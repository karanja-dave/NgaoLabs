{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c194ca5",
   "metadata": {},
   "source": [
    "# Loan Default Prediction:Exploratory Data Analysis\n",
    "\n",
    "**Authors:**  \n",
    "- Dave Karanja\n",
    "- Jedidiah Wambui\n",
    "\n",
    "**Project Repository:**  \n",
    "GitHub: https://github.com/karanja-dave/NgaoLabs/tree/dave/Loan_default_prediction\n",
    "\n",
    "NB: Ensure all dataset files are stored in a data folder in unzipped format. This prevents file path errors and ensures smooth, consistent data loading during analysis.\n",
    "\n",
    "**Date:**  \n",
    "15 February 2026  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Objective\n",
    "\n",
    "This notebook presents the exploratory data analysis conducted prior to model development.  \n",
    "The purpose of the analysis is to:\n",
    "\n",
    "- Understand data structure and feature distributions  \n",
    "- Detect missing values and anomalies  \n",
    "- Assess categorical imbalance  \n",
    "- Examine relationships with the target variable  \n",
    "- Inform preprocessing and model selection decisions  \n",
    "\n",
    "## Objectives\n",
    "\n",
    "### General Objective\n",
    "- Prepare and analyze customer demographics, performance, and loan history to predict loan default risk.\n",
    "\n",
    "### Specific Objectives\n",
    "1. **Data Cleaning:** Handle missing values, convert dates, and create basic features like age and relative risk.  \n",
    "2. **Exploratory Data Analysis (EDA):** Explore distributions, identify outliers, and visualize patterns in the data.  \n",
    "3. **Feature Engineering:** Generate features such as previous loan defaults, default rates, and risk ratios.  \n",
    "4. **Encoding & Dataset Preparation:** Encode categorical variables and structure datasets for modeling.  \n",
    "5. **Data Integration:** Merge demographics, performance, and previous loan datasets into a single consolidated dataset for analysis and modeling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5ca712",
   "metadata": {},
   "outputs": [],
   "source": [
    "###import libraries for\n",
    "#EDA\n",
    "import pandas as pd \n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "#data preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d632a0d",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "In these section, all three datasets: demographics, performance, and previous loans, were carefully loaded and examined for missing values and inconsistencies. Columns with more than 50% missing data were dropped, while important categorical fields like employment status were imputed with the most frequent values. Date fields, such as birthdates and loan approval or repayment dates, were converted to datetime format, allowing the creation of new features including age, loan approval periods, and repayment delays. Additionally, relative risk was calculated to capture the ratio of current to average historical loan amounts per customer. Unnecessary columns, such as temporary calculation fields and original date columns after feature engineering, were removed to ensure a clean, consistent dataset ready for exploratory analysis, visualization, and subsequent modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ccaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wrangle func \n",
    "#demographics \n",
    "def wrangle_demo(path):\n",
    "    # load data \n",
    "    df=pd.read_csv(path)\n",
    "\n",
    "    # drop features with high null count >0.5\n",
    "    mask_na=df.isna().mean()>0.5    \n",
    "    df.drop(columns=df.columns[mask_na],inplace=True)\n",
    "\n",
    "    # impute missing values in employment status with mode employment status \n",
    "    mode_value = df['employment_status_clients'].mode()[0]\n",
    "    df['employment_status_clients'] = df['employment_status_clients'].fillna(mode_value)\n",
    "\n",
    "    # convert `datatype` to date\n",
    "    df['birthdate'] = pd.to_datetime(df['birthdate'])\n",
    "    # get everyone's age\n",
    "    df['age']=(pd.Timestamp.today()-df['birthdate']).dt.days//365\n",
    "    # drop `datatype` col \n",
    "    df.drop(columns='birthdate',inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "#performance data \n",
    "def wrangle_perf(path):\n",
    "    # load data \n",
    "    df=pd.read_csv(path)\n",
    "    # drop features with high null count\n",
    "    mask_na=df.isna().mean()>0.5    \n",
    "    df.drop(columns=df.columns[mask_na],inplace=True)\n",
    "    # # convert `approvedate`&`creationdate` to date type\n",
    "    # df['approveddate'] = pd.to_datetime(df['approveddate'])\n",
    "    # df['creationdate'] = pd.to_datetime(df['creationdate'])\n",
    "\n",
    "    # for now drop the time date cols \n",
    "    df.drop(columns=['approveddate','creationdate'],inplace=True)\n",
    "\n",
    "    # relative risk\n",
    "    # ompute average historical loan amount per customer\n",
    "    df['avg_loan_amt'] = df.groupby('customerid')['loanamount'].transform('mean')\n",
    "    # compute relative risk : loan to avergae loan ration/\n",
    "    df['relative_risk'] = df['loanamount'] / df['avg_loan_amt']\n",
    "    # drop `avg_loan_amt` col \n",
    "    df.drop(columns=['avg_loan_amt'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "##previous loans data \n",
    "def wrangle_prev_loans(path):\n",
    "    # load data \n",
    "    df=pd.read_csv(path)\n",
    "    #deal with high null counts\n",
    "    mask_na=df.isna().mean()>0.5\n",
    "    df.drop(columns=df.columns[mask_na],inplace=True)\n",
    "\n",
    "    # select only date cols \n",
    "    mask_date=df.select_dtypes(include='object').drop(columns='customerid').columns\n",
    "    # convert dates to date data type \n",
    "    for col in mask_date:\n",
    "        df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "    \n",
    "    # loan approval period in hours: most loans are approved within a day\n",
    "    df['approval_period'] = (df['approveddate'] - df['creationdate']).dt.total_seconds() / 3600\n",
    "\n",
    "    # load default\n",
    "    # loan repayment perido (days) : first loan\n",
    "    df['first_delay'] = (df['firstrepaiddate'] - df['firstduedate']).dt.days \n",
    "    # subsequent loan delays (days)\n",
    "    df['loan_delays'] = (df['closeddate'] - df['approveddate']).dt.days - df['termdays']\n",
    "    # defaulted loans : Im using loans beyond dues date are defaulted \n",
    "    df['prev_loan_default'] = ((df['first_delay'] > 0) | (df['loan_delays'] > 0)).astype(int)\n",
    "    #drop date cols, `first_delay` and `loan_delays`\n",
    "    date_cols=df.select_dtypes(include='datetime').columns\n",
    "    df.drop(columns=list(date_cols) + ['first_delay','loan_delays'],inplace=True)\n",
    "\n",
    "    # relative risk\n",
    "    # ompute average historical loan amount per customer\n",
    "    df['avg_loan_amt'] = df.groupby('customerid')['loanamount'].transform('mean')\n",
    "    # compute relative risk : loan to avergae loan ration/\n",
    "    df['relative_risk'] = df['loanamount'] / df['avg_loan_amt']\n",
    "    # drop `avg_loan_amt` col \n",
    "    df.drop(columns=['avg_loan_amt'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1a314",
   "metadata": {},
   "source": [
    "The data wrangling functions for the datasets were developed based on insights from the initial EDA. The EDA guided decisions such as which columns to drop due to high missing values, which categorical fields required imputation, how to handle date columns, and which new features should be created to enhance the dataset for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87240226",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data sets\n",
    "##demographics data\n",
    "demo_train=wrangle_demo('data/traindemographics.csv')\n",
    "demo_test=wrangle_demo('data/testdemographics.csv')\n",
    "# performance data \n",
    "perf_train=wrangle_perf('data/trainperf.csv')\n",
    "perf_test=wrangle_perf('data/testperf.csv')\n",
    "\n",
    "# previous loans data\n",
    "prev_loans_train=wrangle_prev_loans(\"data/trainprevloans.csv\") \n",
    "prev_loans_test=wrangle_prev_loans('data/testprevloans.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168ec23f",
   "metadata": {},
   "source": [
    "The datasets were loaded using the wrangling functions to ensure consistent formatting, feature creation and that the data was prepared for effective use in analysis and model building "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b512c9c",
   "metadata": {},
   "source": [
    "# Explanatory Data Analysis (EDA)\n",
    "EDA was performed to understand the structure, distribution, and quality of the datasets. This included examining numerical and categorical variables, detecting missing values, identifying outliers, and visualizing patterns. The insights gained from EDA guided feature engineering, data cleaning decisions, and informed the preparation of the datasets for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676eac3a",
   "metadata": {},
   "source": [
    "## Demographics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b6400",
   "metadata": {},
   "outputs": [],
   "source": [
    "### EDA \n",
    "##demographics \n",
    "demo_train.info()\n",
    "demo_test.info()\n",
    "demo_train.head()\n",
    "demo_test.head()\n",
    "\n",
    "# check for missing values\n",
    "demo_train.isna().mean()>0.5\n",
    "demo_train.isna().sum()\n",
    "demo_test.isna().mean()>0.5\n",
    "demo_test.isna().sum()\n",
    "\n",
    "# dealing with cardinality\n",
    "cate_var = demo_train.select_dtypes(include=\"object\").drop(columns='customerid')\n",
    "print(\"Cardinality count for categorical variables:\\n\")\n",
    "cate_var.nunique()\n",
    "print('\\n `birthdate` will be used to get ages of bank clients. \\n Transform it to date instead \\n')\n",
    "print(\"\\n `customerid` is the primary key to be used to merge the 3 data-sets \\n\")\n",
    "print(\"\\n No high nor low cardinality is observed \\n\")\n",
    "# detailed info on categorical variables\n",
    "for col in cate_var:\n",
    "    print(f\"{col}: {demo_train[col].nunique()} unique values\")\n",
    "    print(demo_train[col].value_counts(dropna=False))\n",
    "    print(\"-\"*50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc5c6c4",
   "metadata": {},
   "source": [
    "After applying the data wrangling functions, the demographics datasets were cleaned and ready for analysis. Prior to cleaning, several columns contained missing values and inconsistent formats. The wrangling functions successfully handled these issues by imputing missing employment statuses, converting birthdate to age, and dropping columns with excessive missing data. After wrangling EDA showed that both datasets contained 7 columns with no missing values. Categorical analysis revealed that `bank_account_type` had 3 unique values, `bank_name_clients` had 18, and `employment_status_clients `had 6. The distributions indicated that most clients held a savings account, most banked with GT Bank, and the majority were permanently employed. There were no extreme cardinality issues, confirming that the datasets were well prepared for encoding and modeling. These results demonstrate that the wrangling functions effectively cleaned and structured the data as intended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2b8e9a",
   "metadata": {},
   "source": [
    "### Visualization: Demographics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c627ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##visulaization\n",
    "# histogram : Distribution of numerical cols \n",
    "num_cols = demo_train.select_dtypes(include=[\"int64\",\"float64\"]).columns\n",
    "\n",
    "demo_train[num_cols].hist(bins=30, figsize=(14,10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#age distribution with employment status\n",
    "demo_train.boxplot(column='age', by='employment_status_clients', figsize=(12,6))\n",
    "plt.title('Age Distribution by Employment Status')\n",
    "plt.suptitle('')  \n",
    "plt.ylabel('Age')\n",
    "plt.xlabel('Employment Status')\n",
    "plt.show()\n",
    "\n",
    "# employment dist with bank type \n",
    "sns.countplot(data=demo_train, x='bank_account_type', hue='employment_status_clients')\n",
    "plt.title('Employment Status Counts by Bank Account Type')\n",
    "plt.xlabel('Bank Account Type')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# barchart\n",
    "# func to plot barchart \n",
    "def plot_barh(df, col, ax):\n",
    "    df[col].value_counts().plot(\n",
    "        kind='bar',\n",
    "        ax=ax,\n",
    "        color='skyblue'\n",
    "    )\n",
    "    ax.set_title(f'Distribution of {col}')\n",
    "    ax.set_xlabel(\"Count\")\n",
    "    ax.set_ylabel(col)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "plot_barh(demo_train, 'bank_account_type', axes[0])\n",
    "plot_barh(demo_train, 'bank_name_clients', axes[1])\n",
    "plot_barh(demo_train, 'employment_status_clients', axes[2])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba1b94",
   "metadata": {},
   "source": [
    "Visualization was used to better understand the distribution and relationships of numerical and categorical variables in the cleaned demographics dataset. Histograms showed the overall distribution of numerical features, including age and GPS coordinates, highlighting central tendencies and spread. A boxplot of age by employment status revealed that permanent employees tended to be older, while students and contract workers were younger. Count plots examined the relationship between employment status and bank account type, showing that most permanent employees held savings accounts across different banks. Additional bar charts illustrated the distribution of key categorical variables: bank_account_type, bank_name_clients, and employment_status_clients. These visualizations confirmed patterns observed during EDA, helped identify group differences, and provided a clear basis for feature encoding and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164c688",
   "metadata": {},
   "source": [
    "### Encoding: Demographics\n",
    "Encoding is done to convert categorical data into a numerical format that machine learning models can understand. Most ML algorithms likelogistic regression, linear regression, neural networks, e.t.c, can't work directly with text or string labels. One-hot encoding is used here because the categorical variables in your demographics dataset (`bank_account_type`, `bank_name_clients`, and `employment_status_clients`) are nominal. This means the categories have no inherent order or ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5208d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical encoding\n",
    "# init ohe\n",
    "ohe = OneHotEncoder(drop='first', sparse_output=False)  # drop='first' avoids dummy variable trap for linear models\n",
    "\n",
    "# select categorical columns for OHE\n",
    "ohe_cols = ['bank_account_type', 'employment_status_clients', 'bank_name_clients']\n",
    "\n",
    "# fit & transform train data\n",
    "demo_train_ohe = pd.DataFrame(\n",
    "    ohe.fit_transform(demo_train[ohe_cols]),\n",
    "    columns=ohe.get_feature_names_out(ohe_cols),\n",
    "    index=demo_train.index\n",
    ")\n",
    "\n",
    "# transform test data using the same encoder\n",
    "demo_test_ohe = pd.DataFrame(\n",
    "    ohe.transform(demo_test[ohe_cols]),\n",
    "    columns=ohe.get_feature_names_out(ohe_cols),\n",
    "    index=demo_test.index\n",
    ")\n",
    "\n",
    "# drop original categorical columns and concatenate OHE columns\n",
    "demo_train = pd.concat([demo_train.drop(columns=ohe_cols), demo_train_ohe], axis=1)\n",
    "demo_test = pd.concat([demo_test.drop(columns=ohe_cols), demo_test_ohe], axis=1)\n",
    "\n",
    "demo_train.info()\n",
    "demo_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1058459a",
   "metadata": {},
   "source": [
    "The selected categorical columns (`bank_account_type`, `employment_status_clients`, and `bank_name_clients`) were one-hot encoded for both train and test sets. The encoding was applied to the training data using fit_transform, and the same encoder was used to transform the test data, ensuring consistency. After encoding, the original categorical columns were dropped, and the newly created binary columns were concatenated with the datasets. This process ensured that all categorical information was preserved in a numeric format suitable for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d746b1a",
   "metadata": {},
   "source": [
    "### Dealing with outliers: Demographics \n",
    "Outliers are extreme values that deviate significantly from the majority of the data. They can distort statistical analyses, bias model estimates, and reduce predictive accuracy. Detecting and handling outliers ensures that the dataset reflects realistic, reliable values, making the models more robust and the analysis more meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541575d",
   "metadata": {},
   "source": [
    "# outliers in age : age shld be b2n [18,80]\n",
    "demo_train[~demo_train['age'].between(18, 80)]\n",
    "demo_test[~demo_test['age'].between(18, 80)]\n",
    "print(\"\\n No unrealistic ages\")\n",
    "\n",
    "# outliers in location \n",
    "demo_train[\n",
    "    ~(\n",
    "        demo_train['longitude_gps'].between(2, 15) &\n",
    "        demo_train['latitude_gps'].between(4.0, 14)\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6d4e6",
   "metadata": {},
   "source": [
    "Age was checked to ensure it fell within a realistic range of 18 to 80 years. Both the training and test datasets contained only valid ages, so no unrealistic values were present. Geographic coordinates (`longitude_gps` and `latitude_gps`) were also examined against Nigeria’s approximate ranges (longitude 2–15, latitude 4–14). While a few coordinates fell outside these ranges, they were retained because they could represent Nigerians living abroad who still took loans. These checks confirmed that the datasets were largely clean and that extreme or invalid values were either nonexistent or justifiable, supporting reliable analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3132754e",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573d5553",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_train.info()\n",
    "perf_test.info()\n",
    "\n",
    "# check for missing values \n",
    "perf_train.isna().mean()>0.5\n",
    "perf_test.isna().mean()>0.5\n",
    "perf_train.isna().sum()\n",
    "perf_test.isna().sum()\n",
    "\n",
    "# dealing with cardinality \n",
    "perf_train.select_dtypes(include='object')['good_bad_flag'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33815598",
   "metadata": {},
   "source": [
    "After applying the performance data wrangling function, both performance datasets were cleaned and structured for analysis. All columns were complete with no missing values, confirming that the wrangling function successfully handled nulls and unnecessary features. The date columns (`approveddate` and `creationdate`) were considered obsolete for the analysis (the test data had parsing issues, likely due to entry errors, making it impossible to convrt to date types. To maintain consistency, the same columns were also dropped in the training data even though it had no parsing issues). The target variable, `good_bad_flag`, is categorical with two classes representing loan repayment status and was retained in the training set since it is absent in the test set. Its low cardinality makes it suitable for classification tasks. Overall, the wrangling function effectively prepared the performance data, ensuring it is clean, consistent, and ready for feature integration and predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc84ac",
   "metadata": {},
   "source": [
    "### Visualization: Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f612ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#hist:dist of numerical variables\n",
    "num_cols = perf_train.select_dtypes(include=[\"int64\",\"float64\"]).drop(columns='systemloanid').columns\n",
    "\n",
    "perf_train[num_cols].hist(bins=30, figsize=(14,10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# barchart: dist for categorical var\n",
    "(perf_train['good_bad_flag'].value_counts().\n",
    "    plot( kind='bar',figsize=(6,4),color='skyblue', title=f'Distribution of Loan Defaulting'))\n",
    "\n",
    "plt.xlabel(\"Defaulting\")\n",
    "plt.ylabel('Count') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac28de9a",
   "metadata": {},
   "source": [
    "Visualizations were used to examine the distribution of numerical and categorical features in the cleaned performance dataset. Histograms of numerical variables, showed the spread, central tendencies, and potential skewness, helping identify patterns in customer loan behavior. A bar chart of the target variable `good_bad_flag` revealed that out of 4,368 loans in the training set, 3,416 were classified as “Good” and 952 as “Bad,” indicating a moderate imbalance. These visualizations provided a clear understanding of feature distributions, confirmed that the data was properly cleaned, and highlighted patterns that could be important for predictive modeling. These visualizations provided a clear understanding of feature distributions, confirmed that the data was properly cleaned, and highlighted patterns that could be important for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a6a133",
   "metadata": {},
   "source": [
    "### label encoding: perfomance data \n",
    "Label encoding is used to convert categorical variables into numeric values that machine learning models can process. It is applied to ordinal variables, where the categories have a natural order or represent classes. The target variable `good_bad_flag` will be label-encoded so that “Good” and “Bad” could be represented as 0 and 1 for classification modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a1336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encoding\n",
    "y_train=perf_train['good_bad_flag']\n",
    "# target encoding\n",
    "le_target=LabelEncoder() \n",
    "y_train=le_target.fit_transform(y_train)\n",
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86a306",
   "metadata": {},
   "source": [
    "## Previous Loans data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4daf94de",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_loans_train.info()\n",
    "prev_loans_test.info()\n",
    "\n",
    "# check for null counts \n",
    "prev_loans_train.isna().mean()>0.5\n",
    "prev_loans_test.isna().mean()>0.5\n",
    "\n",
    "# check if dates are of type date : no output as dates are dropped\n",
    "prev_loans_train.select_dtypes(include='datetime').head()\n",
    "prev_loans_test.select_dtypes(include='datetime').head()\n",
    "\n",
    "#aggregate previous loan defaults per customer\n",
    "customer_features = prev_loans_train.groupby('customerid').agg(\n",
    "    num_bad_loans=('prev_loan_default', 'sum'),          # total previous defaults\n",
    "    default_rate_prev=('prev_loan_default', 'mean'),    # fraction of loans defaulted\n",
    "    recent_default_flag=('prev_loan_default', 'last')   # default status of most recent loan\n",
    ").reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635cb40f",
   "metadata": {},
   "source": [
    "After applying the wrangling function, the previous loans datasets were cleaned and structured for analysis. Columns with excessive missing values were removed, and date columns were converted to datetime format to calculate useful features, such as `approval_period`, `first_delay`, and `loan_delays`. Using these, a new binary feature `prev_loan_default` was created to flag whether a loan was defaulted. After these computations, all original date columns, along with intermediate calculation columns (`first_delay`, `loan_delays`), were dropped to simplify the dataset. Additionally, a `relative_risk` feature was computed by comparing each loan amount to the customer’s historical average, capturing risk behavior. Finally, the dataset was aggregated per customer, producing three key features: `num_bad_loans` (total defaults), `default_rate_prev` (fraction of loans defaulted), and `recent_default_flag` (status of the most recent loan). These steps ensured that the previous loans data was clean, consistent, and summarized effectively for predictive modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2234bec",
   "metadata": {},
   "source": [
    "### visulaization : Previos loans \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721a7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist :dist of all numerical vars \n",
    "num_cols = prev_loans_train.select_dtypes(include=[\"int64\",\"float64\"]).drop(columns='systemloanid').columns\n",
    "\n",
    "prev_loans_train[num_cols].hist(bins=30, figsize=(14,10))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6416de2b",
   "metadata": {},
   "source": [
    "Most features in the previous loans dataset were numerical, so their distributions were examined using histograms. This allowed observation of the spread, central tendencies, and potential skewness in features such as num_bad_loans, default_rate_prev, and recent_default_flag. The histograms provided a clear view of customer loan behaviors and highlighted patterns that could inform modeling decisions, such as identifying customers with higher historical default rates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dccebf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
